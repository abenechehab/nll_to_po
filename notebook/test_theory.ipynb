{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "# sns.set_palette(\"colorblind\")\n",
    "# sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38422cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from nll_to_po.models.dn_policy import MLPPolicy\n",
    "from nll_to_po.training.utils import train_single_policy, setup_logger\n",
    "import nll_to_po.training.loss as L\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c626ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"wandb\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"  # Suppress WandB output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fa8e6",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc7cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "n_experiments = 1\n",
    "n_updates = 500\n",
    "learning_rate = 0.001\n",
    "use_wandb = True\n",
    "wandb_project = \"tractable\"\n",
    "\n",
    "# Policy architecture\n",
    "input_dim = 4\n",
    "output_dim = 2\n",
    "hidden_sizes = [64, 64]\n",
    "fixed_logstd = False\n",
    "\n",
    "# Data generating dist q\n",
    "init_dist_loc = 5.0\n",
    "init_dist_scale = 0.75\n",
    "init_dist_n_samples = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63138432",
   "metadata": {},
   "source": [
    "### NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e111816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 13:38:05 - nll_to_po - INFO - %%%%%%%%%%%%%%%%%%%\n",
      "config:\n",
      "{'batch_size': 25, 'fixed_logstd': False, 'init_dist_loc': 5.0, 'init_dist_scale': 0.75, 'init_dist_n_samples': 25, 'learning_rate': 0.001, 'loss': 'NLL'}\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "2025-08-13 13:38:05 - nll_to_po - INFO - %%%%%%%%%%%%%%%%%%%\n",
      "config:\n",
      "{'batch_size': 25, 'fixed_logstd': False, 'init_dist_loc': 5.0, 'init_dist_scale': 0.75, 'init_dist_n_samples': 25, 'learning_rate': 0.001, 'loss': 'NLL'}\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "2025-08-13 13:38:05 - nll_to_po - INFO - Starting training for 500 epochs\n",
      "2025-08-13 13:38:05 - nll_to_po - INFO - Starting training for 500 epochs\n",
      "Training epochs:   0%|          | 0/500 [00:00<?, ?it/s]2025-08-13 13:38:05 - nll_to_po - INFO - %%%%%%%%%%%%%%%%%%%\n",
      "config:\n",
      "{'batch_size': 25, 'fixed_logstd': False, 'init_dist_loc': 5.0, 'init_dist_scale': 0.75, 'init_dist_n_samples': 25, 'learning_rate': 0.001, 'loss': 'NLL'}\n",
      "%%%%%%%%%%%%%%%%%%%%\n",
      "2025-08-13 13:38:05 - nll_to_po - INFO - Starting training for 500 epochs\n",
      "2025-08-13 13:38:05 - nll_to_po - INFO - Starting training for 500 epochs\n",
      "Training epochs: 100%|██████████| 500/500 [00:01<00:00, 448.14it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp_idx in range(n_experiments):\n",
    "    policy = MLPPolicy(input_dim, output_dim, hidden_sizes, fixed_logstd)\n",
    "\n",
    "    # Generate new random data for each experiment\n",
    "    X = torch.randn(1, input_dim)\n",
    "    mean_y = torch.ones((1, output_dim)) * init_dist_loc\n",
    "    y = mean_y + torch.randn(init_dist_n_samples, output_dim) * init_dist_scale\n",
    "    X = X.repeat(init_dist_n_samples, 1)  # Repeat X for each sample\n",
    "    batch_size = X.shape[0]\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_function = L.NLL()\n",
    "\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"fixed_logstd\": fixed_logstd,\n",
    "        \"init_dist_loc\": init_dist_loc,\n",
    "        \"init_dist_scale\": init_dist_scale,\n",
    "        \"init_dist_n_samples\": init_dist_n_samples,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"loss\": loss_function.name,\n",
    "    }\n",
    "    if use_wandb:\n",
    "        wandb_run = wandb.init(\n",
    "            project=wandb_project,\n",
    "            config=config,\n",
    "        )\n",
    "    else:\n",
    "        wandb_run = None\n",
    "    logger, _, ts_writer = setup_logger(\n",
    "        logger_name=\"nll_to_po\",\n",
    "        log_dir=\"../logs\",\n",
    "        env_id=\"test_theory\",\n",
    "        exp_name=f\"NLL_{exp_idx}\",\n",
    "    )\n",
    "    # Log the configuration\n",
    "    logger.info(f\"%%%%%%%%%%%%%%%%%%%\\nconfig:\\n{config}\\n%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    # Run comparison\n",
    "    train_single_policy(\n",
    "        policy=policy,\n",
    "        train_dataloader=train_dataloader,\n",
    "        loss_function=loss_function,\n",
    "        n_updates=n_updates,\n",
    "        learning_rate=learning_rate,\n",
    "        wandb_run=wandb_run,\n",
    "        tensorboard_writer=ts_writer,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90037d16",
   "metadata": {},
   "source": [
    "### PG + entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13378657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   0%|          | 0/500 [00:00<?, ?it/s]/mnt/data_2/abenechehab/micromamba/envs/nllpo/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([25, 2])) that is different to the input size (torch.Size([5, 25, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training epochs: 100%|██████████| 500/500 [00:01<00:00, 336.06it/s]\n"
     ]
    }
   ],
   "source": [
    "n_generations = 5\n",
    "use_rsample = True\n",
    "reward_transform = \"normalize\"\n",
    "entropy_weight = 0.01\n",
    "\n",
    "for _ in range(n_experiments):\n",
    "    policy = MLPPolicy(input_dim, output_dim, hidden_sizes, fixed_logstd)\n",
    "\n",
    "    # Generate new random data for each experiment\n",
    "    X = torch.randn(1, input_dim)\n",
    "    mean_y = torch.ones((1, output_dim)) * init_dist_loc\n",
    "    y = mean_y + torch.randn(init_dist_n_samples, output_dim) * init_dist_scale\n",
    "    X = X.repeat(init_dist_n_samples, 1)  # Repeat X for each sample\n",
    "    batch_size = X.shape[0]\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_function = L.PO_Entropy(\n",
    "        n_generations=n_generations,\n",
    "        use_rsample=use_rsample,\n",
    "        reward_transform=reward_transform,\n",
    "        entropy_weight=entropy_weight,\n",
    "    )\n",
    "\n",
    "    wandb_run = wandb.init(\n",
    "        project=wandb_project,\n",
    "        config={\n",
    "            \"batch_size\": batch_size,\n",
    "            \"fixed_logstd\": fixed_logstd,\n",
    "            \"init_dist_loc\": init_dist_loc,\n",
    "            \"init_dist_scale\": init_dist_scale,\n",
    "            \"init_dist_n_samples\": init_dist_n_samples,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"loss\": loss_function.name,\n",
    "            \"n_generations\": n_generations,\n",
    "            \"use_rsample\": use_rsample,\n",
    "            \"reward_transform\": reward_transform,\n",
    "            \"entropy_weight\": entropy_weight,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Run comparison\n",
    "    train_single_policy(\n",
    "        policy=policy,\n",
    "        train_dataloader=train_dataloader,\n",
    "        loss_function=loss_function,\n",
    "        n_updates=n_updates,\n",
    "        learning_rate=learning_rate,\n",
    "        wandb_run=wandb_run,\n",
    "    )\n",
    "\n",
    "    wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94262a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nllpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
